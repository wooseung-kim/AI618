{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Self-Attention + ControlNet\n",
    "This notebook supports image generation using the Mutual Self-Attention + ControlNet pipeline proposed in the report.\n",
    "\n",
    "The model that supports ControlNet, `MasaCtrlControlNetPipeline`, is implemented in `diffuser_utils.py`.\n",
    "\n",
    "\n",
    "The code work as follows:\n",
    "\n",
    "You first register the `MutualSelfAttention` editor to the `MasaCtrlControlNetPipeline` using the `register_attention_editor_diffusers` function.\n",
    "Then, if you generate an image using a paired prompt in format ['source prompt', 'edited prompt'] along with a conditioning input image, the output will be saved in the `workdir/exp` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Self-Attention + ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diffusers import DDIMScheduler, ControlNetModel\n",
    "\n",
    "from MasaCtrl.masactrl.diffuser_utils import MasaCtrlPipeline, MasaCtrlControlNetPipeline\n",
    "from MasaCtrl.masactrl.masactrl_utils import AttentionBase\n",
    "from MasaCtrl.masactrl.masactrl_utils import regiter_attention_editor_diffusers\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\")\n",
    "\n",
    "model = MasaCtrlControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, scheduler=scheduler, cross_attention_kwargs={\"scale\": 0.5}).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistent synthesis with MasaCtrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MasaCtrl.masactrl.masactrl import MutualSelfAttentionControl\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n",
    "out_dir = \"./workdir/exp/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "prompts = [\n",
    "    \"1boy, casual, outdoors, standing\",  # source prompt\n",
    "    \"1boy, casual, outdoors, dancing\"  # target prompt\n",
    "]\n",
    "\n",
    "condition_image = \"dataset/poses/dance_03.png\"\n",
    "# load the condition image\n",
    "condition_image = read_image(condition_image).float() / 255.0\n",
    "# rgba to rgb conversion\n",
    "if condition_image.shape[0] == 4:\n",
    "    condition_image = condition_image[:3, :, :]\n",
    "    # resize to 512x512\n",
    "condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "condition_image = condition_image.to(device)\n",
    "zero_condition = torch.zeros_like(condition_image)\n",
    "condition = torch.cat([zero_condition, condition_image], dim=0)  # concatenate the condition image and zero condition\n",
    "\n",
    "# initialize the noise map\n",
    "start_code = torch.randn([1, 4, 64, 64], device=device)\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference the synthesized image without MasaCtrl\n",
    "editor = AttentionBase()\n",
    "regiter_attention_editor_diffusers(model, editor)\n",
    "image_ori = model(prompts, controlnet_conditioning=condition, latents=start_code, guidance_scale=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference the synthesized image with MasaCtrl\n",
    "STEP = 4\n",
    "LAYER = 10\n",
    "\n",
    "# hijack the attention module\n",
    "editor = MutualSelfAttentionControl(STEP, LAYER)\n",
    "regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "# inference the synthesized image\n",
    "image_masactrl = model(prompts, controlnet_conditioning=condition, latents=start_code, guidance_scale=7.5)[-1:]\n",
    "\n",
    "# save the synthesized image\n",
    "out_image = torch.cat([image_ori, image_masactrl], dim=0)\n",
    "save_image(out_image, os.path.join(out_dir, f\"all_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[0], os.path.join(out_dir, f\"source_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[1], os.path.join(out_dir, f\"without_step{STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[2], os.path.join(out_dir, f\"masactrl_step{STEP}_layer{LAYER}.png\"))\n",
    "\n",
    "print(\"Syntheiszed images are saved in\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diffusers import DDIMScheduler, ControlNetModel\n",
    "\n",
    "from MasaCtrl.masactrl.diffuser_utils import MasaCtrlPipeline, MasaCtrlControlNetPipeline\n",
    "from MasaCtrl.masactrl.masactrl_utils import AttentionBase\n",
    "from MasaCtrl.masactrl.masactrl_utils import regiter_attention_editor_diffusers\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MasaCtrl.masactrl.masactrl import MutualSelfAttentionControl\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\")\n",
    "\n",
    "model = MasaCtrlControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, scheduler=scheduler, cross_attention_kwargs={\"scale\": 0.5}).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"highly detailed, 1boy, standing, facing camera, full body portrait, full-length portrait\",  # source prompt\n",
    "    \"highly detailed, 1boy, dancing, facing camera, full body portrait, full-length portrait\"  # target prompt\n",
    "]\n",
    "\n",
    "condition_image = \"dataset/poses/dance_01.png\"\n",
    "# load the condition image\n",
    "condition_image = read_image(condition_image).float() / 255.0\n",
    "# rgba to rgb conversion\n",
    "if condition_image.shape[0] == 4:\n",
    "    condition_image = condition_image[:3, :, :]\n",
    "    # resize to 512x512\n",
    "condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "condition_image = condition_image.to(device)\n",
    "zero_condition = torch.zeros_like(condition_image)\n",
    "condition = torch.cat([zero_condition, condition_image], dim=0)  # concatenate the condition image and zero condition\n",
    "\n",
    "# initialize the noise map\n",
    "start_code = torch.randn([1, 4, 64, 64], device=device)\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference the synthesized image without MasaCtrl\n",
    "editor = AttentionBase()\n",
    "regiter_attention_editor_diffusers(model, editor)\n",
    "image_ori = model(prompts, controlnet_conditioning=condition, latents=start_code, guidance_scale=7.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "# Convert the PyTorch tensor to PIL image before saving\n",
    "ToPILImage()(image_ori[0].cpu()).save(\"final_test22/final_test_zero_cond_original.png\")\n",
    "ToPILImage()(image_ori[1].cpu()).save(\"final_test22/final_test_zero_cond_without.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "STEP = 4\n",
    "LAYER = 10\n",
    "# sequential generation\n",
    "\n",
    "folder_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/dataset/poses\"\n",
    "output_folder = \"final_test22\"\n",
    "control_image_files = sorted(glob.glob(f\"{folder_path}/*.png\"))\n",
    "\n",
    "# conditioning image preprocess\n",
    "condition_image = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/dataset/poses/dance_03.png\"\n",
    "\n",
    "\n",
    "\n",
    "for file in control_image_files[:5]:\n",
    "    # load the condition image\n",
    "    condition_image = read_image(file).float() / 255.0\n",
    "    # rgba to rgb conversion\n",
    "    if condition_image.shape[0] == 4:\n",
    "        condition_image = condition_image[:3, :, :]\n",
    "        # resize to 512x512\n",
    "    condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "    condition_image = condition_image.to(device)\n",
    "    zero_condition = torch.zeros_like(condition_image)\n",
    "    condition = torch.cat([zero_condition, condition_image], dim=0)  # concatenate the condition image and zero condition\n",
    "\n",
    "    # inference the synthesized image with MasaCtrl\n",
    "    STEP = 4\n",
    "    LAYER = 10\n",
    "\n",
    "    # hijack the attention module\n",
    "    # editor = MutualSelfAttentionControl(STEP, LAYER)\n",
    "    editor = MutualSelfAttentionControl(STEP, LAYER)\n",
    "    regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "    # inference the synthesized image\n",
    "    image_masactrl = model(prompts, controlnet_conditioning=condition, latents=start_code, guidance_scale=7.5)[-1:]\n",
    "    # Save the edited image\n",
    "    file_name, file_ext = os.path.splitext(os.path.basename(file))\n",
    "    # image_masactrl.save(f\"{output_folder}/final_test_{file_name}.png\")  # with attention hijack\n",
    "    save_image(image_masactrl, f\"{output_folder}/final_test_{file_name}.png\")  # with attention hijack\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587aa04bacead72c1ffd459abbe4c8140b72ba2b534b24165b36a2ede3d95042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
