{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MasaCtrl: Tuning-free Mutual Self-Attention Control for Consistent Image Synthesis and Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diffusers import DDIMScheduler, ControlNetModel, StableDiffusionControlNetPipeline\n",
    "from typing import Optional\n",
    "\n",
    "from masactrl.diffuser_utils import MasaCtrlPipeline\n",
    "from masactrl.masactrl_utils import AttentionBase\n",
    "from masactrl.masactrl_utils import regiter_attention_editor_diffusers\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasaCtrlControlNetPipeline(StableDiffusionControlNetPipeline):\n",
    "    \"\"\"\n",
    "    ControlNet-enabled pipeline that retains prompt-to-prompt attention hijacking.\n",
    "    \"\"\"\n",
    "\n",
    "    def enable_attention_control(self, editor):\n",
    "        \"\"\"Register and reset an AttentionStore editor on the UNet\"\"\"\n",
    "        editor.reset()\n",
    "        regiter_attention_editor_diffusers(self, editor)\n",
    "        return self\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt,\n",
    "        control_image,\n",
    "        batch_size=1,\n",
    "        height=512,\n",
    "        width=512,\n",
    "        num_inference_steps=50,\n",
    "        guidance_scale=7.5,\n",
    "        eta=0.0,\n",
    "        editor: Optional[AttentionBase] = None,\n",
    "        return_intermediates=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Hook attention editor if provided\n",
    "        if editor is not None:\n",
    "            self.enable_attention_control(editor)\n",
    "\n",
    "        # Delegate to parent to handle ControlNet conditioning and denoising\n",
    "        return super().__call__(\n",
    "            prompt=prompt,\n",
    "            control_image=control_image,\n",
    "            batch_size=batch_size,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=guidance_scale,\n",
    "            eta=eta,\n",
    "            return_intermediates=return_intermediates,\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'cross_attention_kwargs': {'scale': 0.5}} are not expected by MasaCtrlPipeline and will be ignored.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 13.59it/s]\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 19.81it/s]\n",
      "You have disabled the safety checker for <class '__main__.MasaCtrlControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# Note that you may add your Hugging Face token to get access to the models\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model_path = \"xyn-ai/anything-v4.0\"\n",
    "# model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "model_path = \"./dreambooth_elmo\"\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "model = MasaCtrlPipeline.from_pretrained(model_path, scheduler=scheduler, cross_attention_kwargs={\"scale\": 0.5}).to(device)\n",
    "# model.load_textual_inversion(\"textual_inversion_elmo\")\n",
    "controlnet = ControlNetModel.from_pretrained(\n",
    "    \"lllyasviel/sd-controlnet-openpose\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "model_edit = MasaCtrlControlNetPipeline.from_pretrained(\n",
    "    \"stable-diffusion-v1-5/stable-diffusion-v1-5\",\n",
    "    controlnet=controlnet,\n",
    "    safety_checker=None,\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consistent synthesis with MasaCtrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/hbchoe/workspace/MasaCtrl/masactrl/diffuser_utils.py:144: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  latents_shape = (batch_size, self.unet.in_channels, height//8, width//8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text embeddings : torch.Size([2, 77, 768])\n",
      "latents shape:  torch.Size([2, 4, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler: 100%|██████████| 50/50 [00:18<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MasaCtrl at denoising steps:  [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "MasaCtrl at U-Net layers:  [10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "image must be passed and be one of PIL image, numpy array, torch tensor, list of PIL images, list of numpy arrays or list of torch tensors, but is <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m regiter_attention_editor_diffusers(model, editor)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# inference the synthesized image\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m image_masactrl \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_edit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_image_pose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meditor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meditor\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# save the synthesized image\u001b[39;00m\n\u001b[1;32m     52\u001b[0m out_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([image_ori, image_masactrl], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m, in \u001b[0;36mMasaCtrlControlNetPipeline.__call__\u001b[0;34m(self, prompt, control_image, batch_size, height, width, num_inference_steps, guidance_scale, eta, editor, return_intermediates, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_attention_control(editor)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Delegate to parent to handle ControlNet conditioning and denoising\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_image\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_intermediates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_intermediates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusers/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/ControlNet/diffusers/src/diffusers/pipelines/controlnet/pipeline_controlnet.py:1085\u001b[0m, in \u001b[0;36mStableDiffusionControlNetPipeline.__call__\u001b[0;34m(self, prompt, image, height, width, num_inference_steps, timesteps, sigmas, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, controlnet_conditioning_scale, guess_mode, control_guidance_start, control_guidance_end, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     control_guidance_start, control_guidance_end \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1080\u001b[0m         mult \u001b[38;5;241m*\u001b[39m [control_guidance_start],\n\u001b[1;32m   1081\u001b[0m         mult \u001b[38;5;241m*\u001b[39m [control_guidance_end],\n\u001b[1;32m   1082\u001b[0m     )\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# 1. Check inputs. Raise error if not correct\u001b[39;00m\n\u001b[0;32m-> 1085\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_image_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrol_guidance_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_on_step_end_tensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_guidance_scale \u001b[38;5;241m=\u001b[39m guidance_scale\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clip_skip \u001b[38;5;241m=\u001b[39m clip_skip\n",
      "File \u001b[0;32m~/workspace/ControlNet/diffusers/src/diffusers/pipelines/controlnet/pipeline_controlnet.py:659\u001b[0m, in \u001b[0;36mStableDiffusionControlNetPipeline.check_inputs\u001b[0;34m(self, prompt, image, callback_steps, negative_prompt, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, controlnet_conditioning_scale, control_guidance_start, control_guidance_end, callback_on_step_end_tensor_inputs)\u001b[0m\n\u001b[1;32m    651\u001b[0m is_compiled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(F, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_dot_product_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39meval_frame\u001b[38;5;241m.\u001b[39mOptimizedModule\n\u001b[1;32m    653\u001b[0m )\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet, ControlNetModel)\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_compiled\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet\u001b[38;5;241m.\u001b[39m_orig_mod, ControlNetModel)\n\u001b[1;32m    658\u001b[0m ):\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet, MultiControlNetModel)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_compiled\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrolnet\u001b[38;5;241m.\u001b[39m_orig_mod, MultiControlNetModel)\n\u001b[1;32m    664\u001b[0m ):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/workspace/ControlNet/diffusers/src/diffusers/pipelines/controlnet/pipeline_controlnet.py:775\u001b[0m, in \u001b[0;36mStableDiffusionControlNetPipeline.check_image\u001b[0;34m(self, image, prompt, prompt_embeds)\u001b[0m\n\u001b[1;32m    765\u001b[0m image_is_np_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(image, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m image_is_pil\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_is_tensor\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m image_is_np_list\n\u001b[1;32m    774\u001b[0m ):\n\u001b[0;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    776\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage must be passed and be one of PIL image, numpy array, torch tensor, list of PIL images, list of numpy arrays or list of torch tensors, but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(image)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image_is_pil:\n\u001b[1;32m    780\u001b[0m     image_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: image must be passed and be one of PIL image, numpy array, torch tensor, list of PIL images, list of numpy arrays or list of torch tensors, but is <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "from masactrl.masactrl import MutualSelfAttentionControl\n",
    "\n",
    "\n",
    "seed = 42\n",
    "# seed_everything(seed)\n",
    "\n",
    "out_dir = \"./workdir/masactrl_exp/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# control_image_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/dataset/poses/dance_01.png\"\n",
    "control_image_pose = Image.open(\"/mnt/hdd/hbchoe/workspace/MasaCtrl/dataset/poses/dance_01.png\")\n",
    "\n",
    "\n",
    "# prompts = [\n",
    "#     \"photo of single red <elmo> standng\",  # source prompt\n",
    "#     \"photo of single red <elmo> sitting\"  # target prompt\n",
    "# ]\n",
    "# prompts = [\n",
    "#     \"photo of single red sks doll standng\",  # source prompt\n",
    "#     \"photo of single red sks doll sitting\"  # target prompt\n",
    "# ]\n",
    "\n",
    "prompts = [\n",
    "    \"photo of a boy standng\",  # source prompt\n",
    "    \"photo of a boy sitting\"  # target prompt\n",
    "]\n",
    "\n",
    "# initialize the noise map\n",
    "start_code = torch.randn([1, 4, 64, 64], device=device)\n",
    "start_code = start_code.expand(len(prompts), -1, -1, -1)\n",
    "\n",
    "# inference the synthesized image without MasaCtrl\n",
    "editor = AttentionBase()\n",
    "regiter_attention_editor_diffusers(model, editor)\n",
    "image_ori = model(prompts, latents=start_code, guidance_scale=7.5)\n",
    "\n",
    "# inference the synthesized image with MasaCtrl\n",
    "STEP = 4\n",
    "LAYPER = 10\n",
    "\n",
    "# hijack the attention module\n",
    "editor = MutualSelfAttentionControl(STEP, LAYPER)\n",
    "regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "# inference the synthesized image\n",
    "image_masactrl = model_edit(prompts, control_image=control_image_pose, latents=start_code, guidance_scale=7.5, editor=editor)[-1:]\n",
    "\n",
    "# save the synthesized image\n",
    "out_image = torch.cat([image_ori, image_masactrl], dim=0)\n",
    "save_image(out_image, os.path.join(out_dir, f\"all_step{STEP}_layer{LAYPER}.png\"))\n",
    "save_image(out_image[0], os.path.join(out_dir, f\"source_step{STEP}_layer{LAYPER}.png\"))\n",
    "save_image(out_image[1], os.path.join(out_dir, f\"without_step{STEP}_layer{LAYPER}.png\"))\n",
    "save_image(out_image[2], os.path.join(out_dir, f\"masactrl_step{STEP}_layer{LAYPER}.png\"))\n",
    "\n",
    "print(\"Syntheiszed images are saved in\", out_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "587aa04bacead72c1ffd459abbe4c8140b72ba2b534b24165b36a2ede3d95042"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
