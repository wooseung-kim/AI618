{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a56e6d2-0524-4c2a-9f4c-c2cb798456d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.24.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting lpips\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytorch-fid\n",
      "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from scikit-image) (1.26.4)\n",
      "Collecting scipy>=1.9 (from scikit-image)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: networkx>=2.8 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from scikit-image) (3.2.1)\n",
      "Requirement already satisfied: pillow>=9.1 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from scikit-image) (11.2.1)\n",
      "Requirement already satisfied: imageio>=2.33 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from scikit-image) (2.37.0)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2024.8.30-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from scikit-image) (24.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: torch>=0.4.0 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from lpips) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.2.1 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from lpips) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: filelock in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (4.14.0)\n",
      "Requirement already satisfied: jinja2 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from torch>=0.4.0->lpips) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from sympy==1.13.1->torch>=0.4.0->lpips) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages (from jinja2->torch>=0.4.0->lpips) (3.0.2)\n",
      "Downloading scikit_image-0.24.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.8.30-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: tifffile, scipy, lazy-loader, scikit-image, pytorch-fid, lpips\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [lpips]32m4/6\u001b[0m [pytorch-fid]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed lazy-loader-0.4 lpips-0.1.4 pytorch-fid-0.3.0 scikit-image-0.24.0 scipy-1.13.1 tifffile-2024.8.30\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-image lpips pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe23a10-0ae5-4ab9-9b25-770f1e11b088",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision import transforms as TF\n",
    "from pytorch_fid import fid_score\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "import lpips\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a650606-52fa-4f5c-af34-1fd05a85322b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/hdd/hbchoe/workspace/MasaCtrl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = %pwd\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840f2ab0-852b-4328-b19b-c2294d295074",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = path + \"/dataset/test_output/final_test_original.png\"\n",
    "edit_image_path = path + \"/dataset/test_output/final_test_output.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d6b5847-26fc-41f8-b6f9-d6f7906d771d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/fixed_seed/edit'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir_path = path + \"/sampling/fixed_seed/source\"\n",
    "edit_dir_path = path + \"/sampling/fixed_seed/edit\"\n",
    "edit_dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e24e0-f091-4b98-83a3-4a993fe174fd",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "The result is in [−1,1], but often normalized or scaled to [0,1] or even percentages in practical use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "993323ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b479444-1c50-463b-9aba-f3c3e3f244d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d9911-69cb-4873-8979-d9b51eef84b6",
   "metadata": {},
   "source": [
    "### Indivudual pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4228ae8-40ee-4cce-b97e-6de91d619390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_prompt_clip_score(image: Image.Image, prompt: str):\n",
    "    inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        image_embeds = outputs.image_embeds  # (1, D)\n",
    "        text_embeds = outputs.text_embeds    # (1, D)\n",
    "        similarity = torch.nn.functional.cosine_similarity(image_embeds, text_embeds)\n",
    "    return similarity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2d8ac6-2276-48ed-834e-14dafc7df871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_image_clip_similarity(image1: Image.Image, image2: Image.Image):\n",
    "    inputs1 = clip_processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "    inputs2 = clip_processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image1_embed = clip_model.get_image_features(**inputs1)\n",
    "        image2_embed = clip_model.get_image_features(**inputs2)\n",
    "        similarity = torch.nn.functional.cosine_similarity(image1_embed, image2_embed)\n",
    "    \n",
    "    return similarity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "267065d5-02b7-4fc9-a1c6-a3b3ca9d5954",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/hdd/hbchoe/workspace/MasaCtrl/dataset/test_output/final_test_original.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load images\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image1 \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_image_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m image2 \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(edit_image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Text prompt for the edited image\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sheeprl/lib/python3.9/site-packages/PIL/Image.py:3505\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3502\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3505\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/hdd/hbchoe/workspace/MasaCtrl/dataset/test_output/final_test_original.png'"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "image1 = Image.open(source_image_path).convert(\"RGB\")\n",
    "image2 = Image.open(edit_image_path).convert(\"RGB\")\n",
    "\n",
    "# Text prompt for the edited image\n",
    "target_prompt = \"a boy dancing outdoors\"\n",
    "\n",
    "# Image–Prompt CLIP Score\n",
    "clip_score = compute_image_prompt_clip_score(image2, target_prompt)\n",
    "print(\"CLIP (image-prompt):\", clip_score)\n",
    "\n",
    "# Image–Image CLIP Similarity\n",
    "clip_image_sim = compute_image_image_clip_similarity(image1, image2)\n",
    "print(\"CLIP (image-image):\", clip_image_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07752ea8-5c0b-4b56-a565-ce42858f0fac",
   "metadata": {},
   "source": [
    "### Directory Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adda3150-0d7e-46d2-8571-ec8786c059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_image_text_dir(image_dir, prompt_dict):\n",
    "    scores = []\n",
    "\n",
    "    for fname in tqdm(sorted(os.listdir(image_dir)), desc=\"CLIP image-text\"):\n",
    "        if fname not in prompt_dict:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        text = prompt_dict[fname]\n",
    "\n",
    "        inputs = clip_processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            sim = torch.nn.functional.cosine_similarity(outputs.image_embeds, outputs.text_embeds).item()\n",
    "            scores.append(sim)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cadac3d-c084-4c7f-b440-be5f8562a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_image_image_dir(source_dir, edited_dir):\n",
    "    scores = []\n",
    "\n",
    "    valid_exts = ('.png', '.jpg', '.jpeg')\n",
    "\n",
    "    source_files = filenames = sorted([\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith(valid_exts) and os.path.isfile(os.path.join(source_dir, f))\n",
    "    ])\n",
    "    \n",
    "    edited_files = sorted([\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith(valid_exts) and os.path.isfile(os.path.join(edited_dir, f))\n",
    "    ])\n",
    "\n",
    "    for fname_src, fname_edit in zip(source_files, edited_files):\n",
    "        path1 = os.path.join(source_dir, fname_src)\n",
    "        path2 = os.path.join(edited_dir, fname_edit)\n",
    "\n",
    "        if not os.path.exists(path1) or not os.path.exists(path2):\n",
    "            continue\n",
    "\n",
    "        image1 = Image.open(path1).convert(\"RGB\")\n",
    "        image2 = Image.open(path2).convert(\"RGB\")\n",
    "\n",
    "        inputs1 = clip_processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "        inputs2 = clip_processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat1 = clip_model.get_image_features(**inputs1)\n",
    "            feat2 = clip_model.get_image_features(**inputs2)\n",
    "            sim = torch.nn.functional.cosine_similarity(feat1, feat2).item()\n",
    "            scores.append(sim)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef327374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boy_dancing.png': 'highly detailed, boy, dancing, facing camera, full body portrait, full-length portrait', 'boy_flexing.png': 'highly detailed, boy, flexing, facing camera, full body portrait, full-length portrait', 'boy_jumping.png': 'highly detailed, boy, jumping, facing camera, full body portrait, full-length portrait', 'boy_laying.png': 'highly detailed, boy, laying, facing camera, full body portrait, full-length portrait', 'boy_tposing.png': 'highly detailed, boy, tposing, facing camera, full body portrait, full-length portrait', 'girl_dancing.png': 'highly detailed, girl, dancing, facing camera, full body portrait, full-length portrait', 'girl_flexing.png': 'highly detailed, girl, flexing, facing camera, full body portrait, full-length portrait', 'girl_jumping.png': 'highly detailed, girl, jumping, facing camera, full body portrait, full-length portrait', 'girl_laying.png': 'highly detailed, girl, laying, facing camera, full body portrait, full-length portrait', 'girl_tposing.png': 'highly detailed, girl, tposing, facing camera, full body portrait, full-length portrait', 'man_dancing.png': 'highly detailed, man, dancing, facing camera, full body portrait, full-length portrait', 'man_flexing.png': 'highly detailed, man, flexing, facing camera, full body portrait, full-length portrait', 'man_jumping.png': 'highly detailed, man, jumping, facing camera, full body portrait, full-length portrait', 'man_laying.png': 'highly detailed, man, laying, facing camera, full body portrait, full-length portrait', 'man_tposing.png': 'highly detailed, man, tposing, facing camera, full body portrait, full-length portrait', 'woman_dancing.png': 'highly detailed, woman, dancing, facing camera, full body portrait, full-length portrait', 'woman_flexing.png': 'highly detailed, woman, flexing, facing camera, full body portrait, full-length portrait', 'woman_jumping.png': 'highly detailed, woman, jumping, facing camera, full body portrait, full-length portrait', 'woman_laying.png': 'highly detailed, woman, laying, facing camera, full body portrait, full-length portrait', 'woman_tposing.png': 'highly detailed, woman, tposing, facing camera, full body portrait, full-length portrait', 'child_dancing.png': 'highly detailed, child, dancing, facing camera, full body portrait, full-length portrait', 'child_flexing.png': 'highly detailed, child, flexing, facing camera, full body portrait, full-length portrait', 'child_jumping.png': 'highly detailed, child, jumping, facing camera, full body portrait, full-length portrait', 'child_laying.png': 'highly detailed, child, laying, facing camera, full body portrait, full-length portrait', 'child_tposing.png': 'highly detailed, child, tposing, facing camera, full body portrait, full-length portrait', 'farmer_dancing.png': 'highly detailed, farmer, dancing, facing camera, full body portrait, full-length portrait', 'farmer_flexing.png': 'highly detailed, farmer, flexing, facing camera, full body portrait, full-length portrait', 'farmer_jumping.png': 'highly detailed, farmer, jumping, facing camera, full body portrait, full-length portrait', 'farmer_laying.png': 'highly detailed, farmer, laying, facing camera, full body portrait, full-length portrait', 'farmer_tposing.png': 'highly detailed, farmer, tposing, facing camera, full body portrait, full-length portrait', 'soldier_dancing.png': 'highly detailed, soldier, dancing, facing camera, full body portrait, full-length portrait', 'soldier_flexing.png': 'highly detailed, soldier, flexing, facing camera, full body portrait, full-length portrait', 'soldier_jumping.png': 'highly detailed, soldier, jumping, facing camera, full body portrait, full-length portrait', 'soldier_laying.png': 'highly detailed, soldier, laying, facing camera, full body portrait, full-length portrait', 'soldier_tposing.png': 'highly detailed, soldier, tposing, facing camera, full body portrait, full-length portrait', 'firefighter_dancing.png': 'highly detailed, firefighter, dancing, facing camera, full body portrait, full-length portrait', 'firefighter_flexing.png': 'highly detailed, firefighter, flexing, facing camera, full body portrait, full-length portrait', 'firefighter_jumping.png': 'highly detailed, firefighter, jumping, facing camera, full body portrait, full-length portrait', 'firefighter_laying.png': 'highly detailed, firefighter, laying, facing camera, full body portrait, full-length portrait', 'firefighter_tposing.png': 'highly detailed, firefighter, tposing, facing camera, full body portrait, full-length portrait', 'pirate_dancing.png': 'highly detailed, pirate, dancing, facing camera, full body portrait, full-length portrait', 'pirate_flexing.png': 'highly detailed, pirate, flexing, facing camera, full body portrait, full-length portrait', 'pirate_jumping.png': 'highly detailed, pirate, jumping, facing camera, full body portrait, full-length portrait', 'pirate_laying.png': 'highly detailed, pirate, laying, facing camera, full body portrait, full-length portrait', 'pirate_tposing.png': 'highly detailed, pirate, tposing, facing camera, full body portrait, full-length portrait', 'basketball player_dancing.png': 'highly detailed, basketball player, dancing, facing camera, full body portrait, full-length portrait', 'basketball player_flexing.png': 'highly detailed, basketball player, flexing, facing camera, full body portrait, full-length portrait', 'basketball player_jumping.png': 'highly detailed, basketball player, jumping, facing camera, full body portrait, full-length portrait', 'basketball player_laying.png': 'highly detailed, basketball player, laying, facing camera, full body portrait, full-length portrait', 'basketball player_tposing.png': 'highly detailed, basketball player, tposing, facing camera, full body portrait, full-length portrait'}\n"
     ]
    }
   ],
   "source": [
    "noun_list = [\"boy\", \"girl\", \"man\", \"woman\", \"child\", \n",
    " \"farmer\", \"soldier\", \"firefighter\", \"pirate\", \"basketball player\"]\n",
    "pose_list = [\"dancing\", \"flexing\", 'jumping', 'laying', 'tposing']\n",
    "prompt_dict = {}\n",
    "\n",
    "for noun in noun_list:\n",
    "    for pose in pose_list:\n",
    "        prompt_dict[f\"{noun}_{pose}.png\"] = f\"highly detailed, {noun}, {pose}, facing camera, full body portrait, full-length portrait\"\n",
    "\n",
    "print(prompt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e07c628-dcce-4011-b649-c7c2bea80c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP image-text: 100%|██████████| 50/50 [00:02<00:00, 23.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP (image-text): 0.3025\n",
      "Average CLIP (image-image): 0.7960\n"
     ]
    }
   ],
   "source": [
    "# Path to folders\n",
    "source_dir = source_dir_path\n",
    "edit_dir = edit_dir_path \n",
    "\n",
    "# # Image-Prompt dict (only needed for image-text CLIP score)\n",
    "# prompt_dict = {\n",
    "#     \"img1.png\": \"a boy standing\",\n",
    "#     \"img2.png\": \"a boy dancing\",\n",
    "#     # ... (one entry per image in edit_dir)\n",
    "# }\n",
    "\n",
    "# Compute scores\n",
    "clip_text_score = compute_clip_image_text_dir(edit_dir, prompt_dict)\n",
    "clip_image_score = compute_clip_image_image_dir(source_dir, edit_dir)\n",
    "\n",
    "print(f\"Average CLIP (image-text): {clip_text_score:.4f}\")\n",
    "print(f\"Average CLIP (image-image): {clip_image_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e6dd53-70b6-468a-9e0c-788417c850bf",
   "metadata": {},
   "source": [
    "## PSNR, LPIPS, SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f81a7c19-c441-475b-aa31-1854c1309a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size=(256, 256), as_tensor=True):\n",
    "    img = Image.open(path).convert(\"RGB\").resize(size, Image.BICUBIC)\n",
    "    if as_tensor:\n",
    "        return T.ToTensor()(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b7626-bf2a-404e-be9a-cb2b0b1644a4",
   "metadata": {},
   "source": [
    "### Individual pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa7bc13-2f84-460d-abee-f2117e4dadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_image(image1_path, image2_path, image_size=(256, 256)):\n",
    "    lpips_model = lpips.LPIPS(net='alex').cuda().eval()\n",
    "\n",
    "    img1 = load_image(image1_path, size=image_size).unsqueeze(0).cuda()\n",
    "    img2 = load_image(image2_path, size=image_size).unsqueeze(0).cuda()\n",
    "\n",
    "    # PSNR / SSIM\n",
    "    np1 = img1.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    np2 = img2.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    psnr = compare_psnr(np1, np2, data_range=1.0)\n",
    "    ssim = compare_ssim(np1, np2, multichannel=True, data_range=1.0, win_size=3) # default: win_size=7\n",
    "    lpips_val = lpips_model(img1, img2).item()\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": psnr,\n",
    "        \"SSIM\": ssim,\n",
    "        \"LPIPS\": lpips_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c9cfc31-c9ce-4dc2-b3ac-54fddfcb3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Image-level PSNR: 13.168018447839662\n",
      "Image-level SSIM: 0.5579255978456787\n",
      "Image-level LPIPS: 0.505652666091919\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_image(source_image_path, edit_image_path)\n",
    "print(\"Image-level PSNR:\", metrics[\"PSNR\"])\n",
    "print(\"Image-level SSIM:\", metrics[\"SSIM\"])\n",
    "print(\"Image-level LPIPS:\", metrics[\"LPIPS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923eb1c-a698-4efd-a897-433d9df3df04",
   "metadata": {},
   "source": [
    "### Directory level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e576885b-1912-4312-8874-428f0c23197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_folder(source_dir, edit_dir, image_size=(256, 256)):\n",
    "    lpips_model = lpips.LPIPS(net='alex').cuda().eval()\n",
    "\n",
    "    psnr_list, ssim_list, lpips_list = [], [], []\n",
    "\n",
    "    # filenames = sorted(os.listdir(source_dir))\n",
    "    valid_exts = ('.png', '.jpg', '.jpeg')\n",
    "    filenames = sorted([\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith(valid_exts) and os.path.isfile(os.path.join(source_dir, f))\n",
    "    ])\n",
    "    for fname in tqdm(filenames, desc=\"Computing PSNR/SSIM/LPIPS\"):\n",
    "        path1 = os.path.join(source_dir, fname)\n",
    "        path2 = os.path.join(edit_dir, fname)\n",
    "        if not os.path.exists(path2):\n",
    "            continue\n",
    "\n",
    "        img1 = load_image(path1, size=image_size).unsqueeze(0).cuda()\n",
    "        img2 = load_image(path2, size=image_size).unsqueeze(0).cuda()\n",
    "\n",
    "        np1 = img1.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "        np2 = img2.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        psnr = compare_psnr(np1, np2, data_range=1.0)\n",
    "        ssim = compare_ssim(np1, np2, multichannel=True, data_range=1.0, win_size=3) # default: win_size=7\n",
    "        lpips_val = lpips_model(img1, img2).item()\n",
    "\n",
    "        psnr_list.append(psnr)\n",
    "        ssim_list.append(ssim)\n",
    "        lpips_list.append(lpips_val)\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": np.mean(psnr_list),\n",
    "        \"SSIM\": np.mean(ssim_list),\n",
    "        \"LPIPS\": np.mean(lpips_list)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83e6ada8-2006-4b7b-b8e0-ebc753dc8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /mnt/hdd/hbchoe/anaconda3/envs/sheeprl/lib/python3.9/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PSNR/SSIM/LPIPS: 100%|██████████| 50/50 [00:02<00:00, 17.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder-level PSNR: 16.414551936020104\n",
      "Folder-level SSIM: 0.7324830875343811\n",
      "Folder-level LPIPS: 0.3856675206124783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_folder(source_dir_path, edit_dir_path)\n",
    "print(\"Folder-level PSNR:\", metrics[\"PSNR\"])\n",
    "print(\"Folder-level SSIM:\", metrics[\"SSIM\"])\n",
    "print(\"Folder-level LPIPS:\", metrics[\"LPIPS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7968a6-2da2-4133-8b2f-8374fe126192",
   "metadata": {},
   "source": [
    "## FID (Directory level only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb994ef-2cb3-445f-b7fe-8e3d8c6d2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid(source_dir, edit_dir, batch_size=4, device=\"cuda\"):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from torchvision import transforms as TF\n",
    "    from pytorch_fid import fid_score\n",
    "\n",
    "    # Monkey-patch ImagePathDataset correctly\n",
    "    def patched_init(self, files, transforms=None):  # Accept `transforms`\n",
    "        self.files = files\n",
    "        self.transforms = TF.Compose([\n",
    "            TF.Resize((299, 299)),\n",
    "            TF.CenterCrop(299),\n",
    "            TF.ToTensor()\n",
    "        ])\n",
    "\n",
    "    fid_score.ImagePathDataset.__init__ = patched_init\n",
    "\n",
    "    # Validate image folder contents\n",
    "    def has_valid_images(folder):\n",
    "        return any(f.lower().endswith((\".png\", \".jpg\", \".jpeg\")) for f in os.listdir(folder))\n",
    "\n",
    "    if not has_valid_images(source_dir) or not has_valid_images(edit_dir):\n",
    "        raise ValueError(\"One or both directories are empty or lack valid image files.\")\n",
    "\n",
    "    # Compute FID\n",
    "    return fid_score.calculate_fid_given_paths(\n",
    "        paths=[source_dir, edit_dir],\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        dims=2048,\n",
    "        num_workers=0  # safer for Jupyter\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fef72f2-a69d-4439-ae1b-5e5a5f341b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 14.59it/s]\n",
      "100%|██████████| 13/13 [00:01<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: 220.9241\n"
     ]
    }
   ],
   "source": [
    "fid = compute_fid(source_dir_path, edit_dir_path)\n",
    "print(f\"FID: {fid:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6885d4d8",
   "metadata": {},
   "source": [
    "# PCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5fd8b-57cc-4963-8172-3bbc38595c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98546165-7b59-4a69-9abc-fd7812fb88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = Path('./videos/video_1/keypoints_json') # directory including keypoints.json files\n",
    "gt_dir   = Path('./videos/video_1/keypoints_json') # directory including keypoints.json files\n",
    "thresh   = 0.2   # Threshold ratio\n",
    "person_idx = 0   # Person index (0-base)\n",
    "\n",
    "pred_files = sorted(pred_dir.glob('*.json'))\n",
    "gt_files   = sorted(gt_dir.glob('*.json'))\n",
    "\n",
    "pair_cnt = min(len(pred_files), len(gt_files))\n",
    "if len(pred_files) != len(gt_files):\n",
    "    warnings.warn(f'Directory sizes differ (pred {len(pred_files)}, gt {len(gt_files)}). Evaluating {pair_cnt} pairs.')\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb5c34-1b7c-4b45-969a-aa426981d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx in range(pair_cnt):\n",
    "    pred_path = pred_files[idx]\n",
    "    gt_path   = gt_files[idx]\n",
    "    try:\n",
    "        # --- Load keypoints ---\n",
    "        with open(pred_path, 'r') as f:\n",
    "            pred_data = json.load(f)\n",
    "        with open(gt_path, 'r') as f:\n",
    "            gt_data = json.load(f)\n",
    "\n",
    "        pred_people = pred_data.get('people', [])\n",
    "        gt_people   = gt_data.get('people', [])\n",
    "\n",
    "        if not pred_people or not gt_people:\n",
    "            raise ValueError('No people in one of the files.')\n",
    "\n",
    "        if person_idx >= len(pred_people) or person_idx >= len(gt_people):\n",
    "            raise IndexError('person_index out of range.')\n",
    "\n",
    "        pred_arr = np.asarray(pred_people[person_idx]['pose_keypoints_2d']).reshape(-1, 3)\n",
    "        gt_arr   = np.asarray(gt_people[person_idx]['pose_keypoints_2d']).reshape(-1, 3)\n",
    "\n",
    "        kp_pred = pred_arr[:, :2]\n",
    "        kp_gt   = gt_arr[:, :2]\n",
    "        conf_gt = gt_arr[:, 2]\n",
    "\n",
    "        # --- Torso length (L-Shoulder 5 ↔ R-Hip 9) ---\n",
    "        torso_len = np.linalg.norm(kp_gt[5] - kp_gt[9])\n",
    "        if torso_len == 0:\n",
    "            raise ValueError('Zero torso length in GT.')\n",
    "\n",
    "        valid = conf_gt > 0.05\n",
    "        dists = np.linalg.norm(kp_pred[valid] - kp_gt[valid], axis=1)\n",
    "        correct = dists < thresh * torso_len\n",
    "        pck_val = correct.mean()\n",
    "\n",
    "        results.append({'index': int(idx),\n",
    "                        'pred_file': str(pred_path.name),\n",
    "                        'gt_file':   str(gt_path.name),\n",
    "                        'PCK': float(pck_val)})\n",
    "    except Exception as e:\n",
    "        results.append({'index': int(idx),\n",
    "                        'pred_file': str(pred_path.name),\n",
    "                        'gt_file':   str(gt_path.name),\n",
    "                        'PCK': np.nan,\n",
    "                        'error': str(e)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bf426",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pck = [r[\"PCK\"] for r in results if isinstance(r[\"PCK\"], float) and not np.isnan(r[\"PCK\"])]\n",
    "mean_pck_list = np.mean(valid_pck) if valid_pck else float(\"nan\")\n",
    "print(f\"\\nMean PCK@{thresh:.1f} = {mean_pck_list*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59639303",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sheeprl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
