{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Sample Generation\n",
    "This file is responsible for generating samples used in the experiments for the report. We set `noun_list` and `pose_list` to make sample dataset.\n",
    "Then we sample 50 samples from each baselines. The results will be saved in `./samples`.\n",
    "Prompt-to-Prompt sampling is handled separately in `sampling_p2p.ipynb` because it requires a different environment setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, repeat\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from diffusers import DDIMScheduler, ControlNetModel, StableDiffusionPipeline, StableDiffusionControlNetPipeline\n",
    "\n",
    "from MasaCtrl.masactrl.diffuser_utils import MasaCtrlPipeline, MasaCtrlControlNetPipeline\n",
    "from MasaCtrl.masactrl.masactrl_utils import AttentionBase\n",
    "from MasaCtrl.masactrl.masactrl_utils import regiter_attention_editor_diffusers\n",
    "from MasaCtrl.masactrl.masactrl import MutualSelfAttentionControl\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "import glob\n",
    "\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "start_code = torch.randn([1, 4, 64, 64], device=device)\n",
    "start_code_masa = start_code.expand(2, -1, -1, -1) # expand to batch size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = [\"boy\", \"girl\", \"man\", \"woman\", \"child\", \n",
    " \"farmer\", \"soldier\", \"firefighter\", \"pirate\", \"basketball player\"]\n",
    "pose_list = [\"dancing\", \"flexing\", 'jumping', 'laying', 'tposing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = [\"controlnet\", \"fixed_seed\", \"masactrl\", \"masactrl_controlnet\", 'test']\n",
    "for baseline in baselines:\n",
    "    if not os.path.exists(f\"sampling/{baseline}\"):\n",
    "        os.mkdir(f\"sampling/{baseline}\")\n",
    "        os.mkdir(f\"sampling/{baseline}/source\")\n",
    "        os.mkdir(f\"sampling/{baseline}/edit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MasaCtrl+ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = \"sampling/pose_selected\"\n",
    "source_path = \"sampling/masactrl_controlnet/source\"\n",
    "edit_path = \"sampling/masactrl_controlnet/edit\"\n",
    "\n",
    "for noun in noun_list:\n",
    "    # model initialization\n",
    "    model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\")\n",
    "    model = MasaCtrlControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, scheduler=scheduler, cross_attention_kwargs={\"scale\": 0.5}).to(device)\n",
    "\n",
    "    # prompt, condition image\n",
    "    prompts = [f\"highly detailed, a {noun}, standing, facing camera, full body portrait, full-length portrait\", f\"highly detailed, {noun}, standing, facing camera, full body portrait, full-length portrait\"]\n",
    "\n",
    "    # print(f\"Source prompt: {prompts[0]}\")\n",
    "    # print(f\"Edit prompt: {prompts[1]}\")\n",
    "\n",
    "    condition_image = f\"{pose_path}/standing.png\"\n",
    "    # load the condition image\n",
    "    condition_image = read_image(condition_image).float() / 255.0\n",
    "    # rgba to rgb conversion\n",
    "    if condition_image.shape[0] == 4:\n",
    "        condition_image = condition_image[:3, :, :]\n",
    "    # resize to 512x512\n",
    "    condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "    condition_image = condition_image.to(device)\n",
    "    zero_condition = torch.zeros_like(condition_image)\n",
    "    condition = torch.cat([zero_condition, condition_image], dim=0)  # concatenate the condition image and zero condition\n",
    "\n",
    "    # generate source image\n",
    "    editor = AttentionBase()\n",
    "    regiter_attention_editor_diffusers(model, editor)\n",
    "    image_ori = model(prompts, controlnet_conditioning=condition, latents=start_code_masa, guidance_scale=7.5)\n",
    "    \n",
    "    # save the original image\n",
    "    for pose in pose_list:\n",
    "        ToPILImage()(image_ori[0].cpu()).save(f\"{source_path}/{noun}_{pose}.png\")\n",
    "    \n",
    "    for pose in pose_list:\n",
    "        prompts = [f\"highly detailed, a {noun}, standing, facing camera, full body portrait, full-length portrait\", f\"highly detailed, {noun}, {pose}, facing camera, full body portrait, full-length portrait\"]\n",
    "        # print(f\"Source prompt: {prompts[0]}\")\n",
    "        # print(f\"Edit prompt: {prompts[1]}\")\n",
    "\n",
    "        condition_image = f\"{pose_path}/{pose}.png\"\n",
    "        # load the condition image\n",
    "        condition_image = read_image(condition_image).float() / 255.0\n",
    "        # rgba to rgb conversion\n",
    "        if condition_image.shape[0] == 4:\n",
    "            condition_image = condition_image[:3, :, :]\n",
    "        # resize to 512x512\n",
    "        condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        condition_image = condition_image.to(device)\n",
    "        zero_condition = torch.zeros_like(condition_image)\n",
    "        condition = torch.cat([zero_condition, condition_image], dim=0)  # concatenate the condition image and zero condition\n",
    "\n",
    "        # generate edited image\n",
    "        editor = MutualSelfAttentionControl(4, 10)\n",
    "        regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "        # inference the synthesized image\n",
    "        image_masactrl = model(prompts, controlnet_conditioning=condition, latents=start_code_masa, guidance_scale=7.5)[-1:]\n",
    "        # Save the edited image\n",
    "        save_image(image_masactrl, f\"{edit_path}/{noun}_{pose}.png\")  # with attention hijack\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MasaCtrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/pose_selected\"\n",
    "source_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/masactrl/source\"\n",
    "edit_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/masactrl/edit\"\n",
    "# control_image_files = sorted(glob.glob(f\"{pose_path}/*.png\"))\n",
    "\n",
    "for noun in noun_list:\n",
    "    # model initialization\n",
    "    model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "    scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "    model = MasaCtrlPipeline.from_pretrained(model_path, scheduler=scheduler, cross_attention_kwargs={\"scale\": 0.5}).to(device)\n",
    "\n",
    "    # prompt, condition image\n",
    "    prompts = [f\"highly detailed, {noun}, standing, facing camera, full body portrait, full-length portrait\", f\"highly detailed, {noun}, standing, facing camera, full body portrait, full-length portrait\"]\n",
    "\n",
    "    # print(f\"Source prompt: {prompts[0]}\")\n",
    "    # print(f\"Edit prompt: {prompts[1]}\")\n",
    "\n",
    "    # generate source image\n",
    "    editor = AttentionBase()\n",
    "    regiter_attention_editor_diffusers(model, editor)\n",
    "    image_ori = model(prompts, latents=start_code_masa, guidance_scale=7.5)\n",
    "    \n",
    "    # save the original image\n",
    "    for pose in pose_list:\n",
    "        ToPILImage()(image_ori[0].cpu()).save(f\"{source_path}/{noun}_{pose}.png\")\n",
    "    \n",
    "    for pose in pose_list:\n",
    "        prompts = [f\"highly detailed, {noun}, standing, facing camera, full body portrait, full-length portrait\", f\"highly detailed, {noun}, {pose}, facing camera, full body portrait, full-length portrait\"]\n",
    "        # print(f\"Source prompt: {prompts[0]}\")\n",
    "        # print(f\"Edit prompt: {prompts[1]}\")\n",
    "\n",
    "        # generate edited image\n",
    "        editor = MutualSelfAttentionControl(4, 10)\n",
    "        regiter_attention_editor_diffusers(model, editor)\n",
    "\n",
    "        # inference the synthesized image\n",
    "        image_masactrl = model(prompts, latents=start_code_masa, guidance_scale=7.5)[-1:]\n",
    "        # Save the edited image\n",
    "        save_image(image_masactrl, f\"{edit_path}/{noun}_{pose}.png\")  # with attention hijack\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/pose_selected\"\n",
    "source_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/fixed_seed/source\"\n",
    "edit_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/fixed_seed/edit\"\n",
    "\n",
    "# model initialization\n",
    "model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "model = StableDiffusionPipeline.from_pretrained(model_path, scheduler=scheduler, safety_checker=None, cross_attention_kwargs={\"scale\": 0.5}).to(device)\n",
    "\n",
    "for noun in noun_list:\n",
    "    # prompt, condition image\n",
    "    prompts = f\"highly detailed, {noun}, standing, facing camera, full body portrait, full-length portrait\"\n",
    "\n",
    "    # print(f\"Source prompt: {prompts}\")\n",
    "\n",
    "    image_ori = model(prompts, latents=start_code, guidance_scale=7.5)[0]\n",
    "    \n",
    "    # save the original image\n",
    "    for pose in pose_list:\n",
    "        image_ori[0].save(f\"{source_path}/{noun}_{pose}.png\")\n",
    "    \n",
    "    for pose in pose_list:\n",
    "        prompts = f\"highly detailed, {noun}, {pose}, facing camera, full body portrait, full-length portrait\"\n",
    "        # print(f\"Edit prompt: {prompts}\")\n",
    "\n",
    "        # edited prompt\n",
    "        image_edit = model(prompts, latents=start_code, guidance_scale=7.5)[0]\n",
    "        # Save the edited image\n",
    "        image_edit[0].save(f\"{edit_path}/{noun}_{pose}.png\")  # Fix applied here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/pose_selected\"\n",
    "source_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/controlnet/source\"\n",
    "edit_path = \"/mnt/hdd/hbchoe/workspace/MasaCtrl/sampling/controlnet/edit\"\n",
    "\n",
    "# model initialization\n",
    "model_path = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-openpose\")\n",
    "model = StableDiffusionControlNetPipeline.from_pretrained(model_path, controlnet=controlnet, scheduler=scheduler, safety_checker=None, cross_attention_kwargs={\"scale\": 0.5}).to(device)\n",
    "\n",
    "for noun in noun_list:\n",
    "    # prompt, condition image\n",
    "    prompts = f\"highly detailed, {noun}, standing, facing camera, full body portrait, full-length portrait\"\n",
    "\n",
    "    # print(f\"Source prompt: {prompts}\")\n",
    "\n",
    "    # condition image\n",
    "    condition_image = f\"{pose_path}/standing.png\"\n",
    "    # load the condition image\n",
    "    condition_image = read_image(condition_image).float() / 255.0\n",
    "    # rgba to rgb conversion\n",
    "    if condition_image.shape[0] == 4:\n",
    "        condition_image = condition_image[:3, :, :]\n",
    "    # resize to 512x512\n",
    "    condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "    condition = condition_image.to(device)\n",
    "\n",
    "\n",
    "    image_ori = model(prompts, image=condition, latents=start_code, guidance_scale=7.5)[0]\n",
    "    \n",
    "    # save the original image\n",
    "    for pose in pose_list:\n",
    "        image_ori[0].save(f\"{source_path}/{noun}_{pose}.png\")\n",
    "    \n",
    "    for pose in pose_list:\n",
    "        prompts = f\"highly detailed, {noun}, {pose}, facing camera, full body portrait, full-length portrait\"\n",
    "        # print(f\"Edit prompt: {prompts}\")\n",
    "\n",
    "        condition_image = f\"{pose_path}/{pose}.png\"\n",
    "        # load the condition image\n",
    "        condition_image = read_image(condition_image).float() / 255.0\n",
    "        # rgba to rgb conversion\n",
    "        if condition_image.shape[0] == 4:\n",
    "            condition_image = condition_image[:3, :, :]\n",
    "        # resize to 512x512\n",
    "        condition_image = F.interpolate(condition_image.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False)\n",
    "        condition = condition_image.to(device)\n",
    "\n",
    "        # edited prompt\n",
    "        image_edit = model(prompts, image=condition, latents=start_code, guidance_scale=7.5)[0]\n",
    "        # Save the edited image\n",
    "        image_edit[0].save(f\"{edit_path}/{noun}_{pose}.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Masactrl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
