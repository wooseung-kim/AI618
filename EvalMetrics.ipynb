{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a56e6d2-0524-4c2a-9f4c-c2cb798456d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (0.22.1)\n",
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting lpips\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytorch-fid\n",
      "  Downloading pytorch_fid-0.3.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.7.1 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from torch==2.7.1->torchvision) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from triton==3.3.1->torch==2.7.1->torchvision) (78.1.1)\n",
      "Collecting scipy>=1.11.4 (from scikit-image)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Downloading tifffile-2025.5.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging>=21 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from scikit-image) (25.0)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.28.1 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from lpips) (4.67.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n",
      "Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "Downloading pytorch_fid-0.3.0-py3-none-any.whl (15 kB)\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2025.5.10-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: tifffile, scipy, lazy-loader, imageio, scikit-image, pytorch-fid, lpips\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [lpips]32m4/7\u001b[0m [scikit-image]\n",
      "\u001b[1A\u001b[2KSuccessfully installed imageio-2.37.0 lazy-loader-0.4 lpips-0.1.4 pytorch-fid-0.3.0 scikit-image-0.25.2 scipy-1.15.3 tifffile-2025.5.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torchvision scikit-image lpips pytorch-fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe23a10-0ae5-4ab9-9b25-770f1e11b088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.io import read_image\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision import transforms as TF\n",
    "from pytorch_fid import fid_score\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "import lpips\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a650606-52fa-4f5c-af34-1fd05a85322b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jenny/MasaCtrl'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = %pwd\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "840f2ab0-852b-4328-b19b-c2294d295074",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = path + \"/dataset/test_output/final_test_original.png\"\n",
    "edit_image_path = path + \"/dataset/test_output/final_test_output.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d6b5847-26fc-41f8-b6f9-d6f7906d771d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jenny/MasaCtrl/output'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_dir_path = path + \"/input\"\n",
    "edit_dir_path = path + \"/output\"\n",
    "edit_dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3e24e0-f091-4b98-83a3-4a993fe174fd",
   "metadata": {},
   "source": [
    "## CLIP\n",
    "The result is in [−1,1], but often normalized or scaled to [0,1] or even percentages in practical use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b479444-1c50-463b-9aba-f3c3e3f244d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d9911-69cb-4873-8979-d9b51eef84b6",
   "metadata": {},
   "source": [
    "### Indivudual pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4228ae8-40ee-4cce-b97e-6de91d619390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_prompt_clip_score(image: Image.Image, prompt: str):\n",
    "    inputs = clip_processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "        image_embeds = outputs.image_embeds  # (1, D)\n",
    "        text_embeds = outputs.text_embeds    # (1, D)\n",
    "        similarity = torch.nn.functional.cosine_similarity(image_embeds, text_embeds)\n",
    "    return similarity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2d8ac6-2276-48ed-834e-14dafc7df871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_image_clip_similarity(image1: Image.Image, image2: Image.Image):\n",
    "    inputs1 = clip_processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "    inputs2 = clip_processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image1_embed = clip_model.get_image_features(**inputs1)\n",
    "        image2_embed = clip_model.get_image_features(**inputs2)\n",
    "        similarity = torch.nn.functional.cosine_similarity(image1_embed, image2_embed)\n",
    "    \n",
    "    return similarity.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "267065d5-02b7-4fc9-a1c6-a3b3ca9d5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP (image-prompt): 0.314338356256485\n",
      "CLIP (image-image): 0.8859783411026001\n"
     ]
    }
   ],
   "source": [
    "# Load images\n",
    "image1 = Image.open(source_image_path).convert(\"RGB\")\n",
    "image2 = Image.open(edit_image_path).convert(\"RGB\")\n",
    "\n",
    "# Text prompt for the edited image\n",
    "target_prompt = \"a boy dancing outdoors\"\n",
    "\n",
    "# Image–Prompt CLIP Score\n",
    "clip_score = compute_image_prompt_clip_score(image2, target_prompt)\n",
    "print(\"CLIP (image-prompt):\", clip_score)\n",
    "\n",
    "# Image–Image CLIP Similarity\n",
    "clip_image_sim = compute_image_image_clip_similarity(image1, image2)\n",
    "print(\"CLIP (image-image):\", clip_image_sim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07752ea8-5c0b-4b56-a565-ce42858f0fac",
   "metadata": {},
   "source": [
    "### Directory Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adda3150-0d7e-46d2-8571-ec8786c059df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_image_text_dir(image_dir, prompt_dict):\n",
    "    scores = []\n",
    "\n",
    "    for fname in tqdm(sorted(os.listdir(image_dir)), desc=\"CLIP image-text\"):\n",
    "        if fname not in prompt_dict:\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir, fname)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        text = prompt_dict[fname]\n",
    "\n",
    "        inputs = clip_processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            sim = torch.nn.functional.cosine_similarity(outputs.image_embeds, outputs.text_embeds).item()\n",
    "            scores.append(sim)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cadac3d-c084-4c7f-b440-be5f8562a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clip_image_image_dir(source_dir, edited_dir):\n",
    "    scores = []\n",
    "\n",
    "    valid_exts = ('.png', '.jpg', '.jpeg')\n",
    "\n",
    "    source_files = filenames = sorted([\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith(valid_exts) and os.path.isfile(os.path.join(source_dir, f))\n",
    "    ])\n",
    "    \n",
    "    edited_files = sorted([\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith(valid_exts) and os.path.isfile(os.path.join(edited_dir, f))\n",
    "    ])\n",
    "\n",
    "    for fname_src, fname_edit in zip(source_files, edited_files):\n",
    "        path1 = os.path.join(source_dir, fname_src)\n",
    "        path2 = os.path.join(edited_dir, fname_edit)\n",
    "\n",
    "        if not os.path.exists(path1) or not os.path.exists(path2):\n",
    "            continue\n",
    "\n",
    "        image1 = Image.open(path1).convert(\"RGB\")\n",
    "        image2 = Image.open(path2).convert(\"RGB\")\n",
    "\n",
    "        inputs1 = clip_processor(images=image1, return_tensors=\"pt\").to(device)\n",
    "        inputs2 = clip_processor(images=image2, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat1 = clip_model.get_image_features(**inputs1)\n",
    "            feat2 = clip_model.get_image_features(**inputs2)\n",
    "            sim = torch.nn.functional.cosine_similarity(feat1, feat2).item()\n",
    "            scores.append(sim)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e07c628-dcce-4011-b649-c7c2bea80c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CLIP image-text: 100%|█████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 220752.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP (image-text): 0.0000\n",
      "Average CLIP (image-image): 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to folders\n",
    "source_dir = source_dir_path\n",
    "edit_dir = edit_dir_path \n",
    "\n",
    "# Image-Prompt dict (only needed for image-text CLIP score)\n",
    "prompt_dict = {\n",
    "    \"img1.png\": \"a boy standing\",\n",
    "    \"img2.png\": \"a boy dancing\",\n",
    "    # ... (one entry per image in edit_dir)\n",
    "}\n",
    "\n",
    "# Compute scores\n",
    "clip_text_score = compute_clip_image_text_dir(edit_dir, prompt_dict)\n",
    "clip_image_score = compute_clip_image_image_dir(source_dir, edit_dir)\n",
    "\n",
    "print(f\"Average CLIP (image-text): {clip_text_score:.4f}\")\n",
    "print(f\"Average CLIP (image-image): {clip_image_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c828770b-84a0-4c6e-9caa-f360a83f3d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28e6dd53-70b6-468a-9e0c-788417c850bf",
   "metadata": {},
   "source": [
    "## PSNR, LPIPS, SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f81a7c19-c441-475b-aa31-1854c1309a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, size=(256, 256), as_tensor=True):\n",
    "    img = Image.open(path).convert(\"RGB\").resize(size, Image.BICUBIC)\n",
    "    if as_tensor:\n",
    "        return T.ToTensor()(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47b7626-bf2a-404e-be9a-cb2b0b1644a4",
   "metadata": {},
   "source": [
    "### Individual pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa7bc13-2f84-460d-abee-f2117e4dadca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_image(image1_path, image2_path, image_size=(256, 256)):\n",
    "    lpips_model = lpips.LPIPS(net='alex').cuda().eval()\n",
    "\n",
    "    img1 = load_image(image1_path, size=image_size).unsqueeze(0).cuda()\n",
    "    img2 = load_image(image2_path, size=image_size).unsqueeze(0).cuda()\n",
    "\n",
    "    # PSNR / SSIM\n",
    "    np1 = img1.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    np2 = img2.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    psnr = compare_psnr(np1, np2, data_range=1.0)\n",
    "    ssim = compare_ssim(np1, np2, multichannel=True, data_range=1.0, win_size=3) # default: win_size=7\n",
    "    lpips_val = lpips_model(img1, img2).item()\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": psnr,\n",
    "        \"SSIM\": ssim,\n",
    "        \"LPIPS\": lpips_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c9cfc31-c9ce-4dc2-b3ac-54fddfcb3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n",
      "Image-level PSNR: 13.168018447839662\n",
      "Image-level SSIM: 0.5579255978456787\n",
      "Image-level LPIPS: 0.505652666091919\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_image(source_image_path, edit_image_path)\n",
    "print(\"Image-level PSNR:\", metrics[\"PSNR\"])\n",
    "print(\"Image-level SSIM:\", metrics[\"SSIM\"])\n",
    "print(\"Image-level LPIPS:\", metrics[\"LPIPS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923eb1c-a698-4efd-a897-433d9df3df04",
   "metadata": {},
   "source": [
    "### Directory level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e576885b-1912-4312-8874-428f0c23197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_folder(source_dir, edit_dir, image_size=(256, 256)):\n",
    "    lpips_model = lpips.LPIPS(net='alex').cuda().eval()\n",
    "\n",
    "    psnr_list, ssim_list, lpips_list = [], [], []\n",
    "\n",
    "    # filenames = sorted(os.listdir(source_dir))\n",
    "    valid_exts = ('.png', '.jpg', '.jpeg')\n",
    "    filenames = sorted([\n",
    "        f for f in os.listdir(source_dir)\n",
    "        if f.lower().endswith(valid_exts) and os.path.isfile(os.path.join(source_dir, f))\n",
    "    ])\n",
    "    for fname in tqdm(filenames, desc=\"Computing PSNR/SSIM/LPIPS\"):\n",
    "        path1 = os.path.join(source_dir, fname)\n",
    "        path2 = os.path.join(edit_dir, fname)\n",
    "        if not os.path.exists(path2):\n",
    "            continue\n",
    "\n",
    "        img1 = load_image(path1, size=image_size).unsqueeze(0).cuda()\n",
    "        img2 = load_image(path2, size=image_size).unsqueeze(0).cuda()\n",
    "\n",
    "        np1 = img1.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "        np2 = img2.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        psnr = compare_psnr(np1, np2, data_range=1.0)\n",
    "        ssim = compare_ssim(np1, np2, multichannel=True, data_range=1.0, win_size=3) # default: win_size=7\n",
    "        lpips_val = lpips_model(img1, img2).item()\n",
    "\n",
    "        psnr_list.append(psnr)\n",
    "        ssim_list.append(ssim)\n",
    "        lpips_list.append(lpips_val)\n",
    "\n",
    "    return {\n",
    "        \"PSNR\": np.mean(psnr_list),\n",
    "        \"SSIM\": np.mean(ssim_list),\n",
    "        \"LPIPS\": np.mean(lpips_list)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e6ada8-2006-4b7b-b8e0-ebc753dc8811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PSNR/SSIM/LPIPS:   0%|                                                                           | 0/4 [00:00<?, ?it/s]/home/jenny/anaconda3/envs/clean310torch/lib/python3.10/site-packages/skimage/metrics/simple_metrics.py:168: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 10 * np.log10((data_range**2) / err)\n",
      "Computing PSNR/SSIM/LPIPS: 100%|███████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 33.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder-level PSNR: inf\n",
      "Folder-level SSIM: 1.0\n",
      "Folder-level LPIPS: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = compute_metrics_folder(source_dir_path, edit_dir_path)\n",
    "print(\"Folder-level PSNR:\", metrics[\"PSNR\"])\n",
    "print(\"Folder-level SSIM:\", metrics[\"SSIM\"])\n",
    "print(\"Folder-level LPIPS:\", metrics[\"LPIPS\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7968a6-2da2-4133-8b2f-8374fe126192",
   "metadata": {},
   "source": [
    "## FID (Directory level only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb994ef-2cb3-445f-b7fe-8e3d8c6d2503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid(source_dir, edit_dir, batch_size=4, device=\"cuda\"):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from torchvision import transforms as TF\n",
    "    from pytorch_fid import fid_score\n",
    "\n",
    "    # Monkey-patch ImagePathDataset correctly\n",
    "    def patched_init(self, files, transforms=None):  # Accept `transforms`\n",
    "        self.files = files\n",
    "        self.transforms = TF.Compose([\n",
    "            TF.Resize((299, 299)),\n",
    "            TF.CenterCrop(299),\n",
    "            TF.ToTensor()\n",
    "        ])\n",
    "\n",
    "    fid_score.ImagePathDataset.__init__ = patched_init\n",
    "\n",
    "    # Validate image folder contents\n",
    "    def has_valid_images(folder):\n",
    "        return any(f.lower().endswith((\".png\", \".jpg\", \".jpeg\")) for f in os.listdir(folder))\n",
    "\n",
    "    if not has_valid_images(source_dir) or not has_valid_images(edit_dir):\n",
    "        raise ValueError(\"One or both directories are empty or lack valid image files.\")\n",
    "\n",
    "    # Compute FID\n",
    "    return fid_score.calculate_fid_given_paths(\n",
    "        paths=[source_dir, edit_dir],\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        dims=2048,\n",
    "        num_workers=0  # safer for Jupyter\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fef72f2-a69d-4439-ae1b-5e5a5f341b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 14.29it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 17.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FID: -0.0002\n"
     ]
    }
   ],
   "source": [
    "fid = compute_fid(source_dir_path, edit_dir_path)\n",
    "print(f\"FID: {fid:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5fd8b-57cc-4963-8172-3bbc38595c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98546165-7b59-4a69-9abc-fd7812fb88fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bb5c34-1b7c-4b45-969a-aa426981d9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clean310torch)",
   "language": "python",
   "name": "clean310torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
